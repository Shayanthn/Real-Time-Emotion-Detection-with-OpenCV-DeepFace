import os
import cv2
import mediapipe as mp
from deepface import DeepFace
import numpy as np
import threading
import turtle
from PIL import ImageFont, ImageDraw, Image
import ctypes

# ğŸ”¹ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Ù‡Ø´Ø¯Ø§Ø±Ù‡Ø§ÛŒ TensorFlow Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"

# ğŸ”¹ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Mediapipe Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¬Ø²Ø¦ÛŒØ§Øª Ú†Ù‡Ø±Ù‡
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.7)

# ğŸ”¹ Ø¯Ø±ÛŒØ§ÙØª Ø±Ø²ÙˆÙ„ÙˆØ´Ù† Ø¨ÛŒØ´ÛŒÙ†Ù‡â€ŒÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· ÙˆØ¨â€ŒÚ©Ù…
cap = cv2.VideoCapture(0)
max_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
max_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS)) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30

print(f"ğŸ“· Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø±Ø²ÙˆÙ„ÙˆØ´Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡: {max_width}x{max_height} @ {fps} FPS")

cap.set(cv2.CAP_PROP_FRAME_WIDTH, max_width)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, max_height)
cap.set(cv2.CAP_PROP_FPS, fps)

# ğŸ”¹ Ù…ØªØºÛŒØ± Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø­Ø³Ø§Ø³ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡
detected_emotion = "Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„..."

# ğŸ”¹ ØªÙ†Ø¸ÛŒÙ… Ù¾Ù†Ø¬Ø±Ù‡â€ŒÛŒ Ù†Ù…Ø§ÛŒØ´ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¨Ø§ `Turtle`
emotion_screen = turtle.Screen()
emotion_screen.setup(400, 200)
emotion_screen.bgcolor("black")
emotion_screen.title("ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª")
emotion_turtle = turtle.Turtle()
emotion_turtle.hideturtle()
emotion_turtle.penup()
emotion_turtle.color("white")

# ğŸ”¹ ØªÙ†Ø¸ÛŒÙ… Ù¾Ù†Ø¬Ø±Ù‡â€ŒÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡
info_screen = turtle.Screen()
info_screen.setup(400, 200)
info_screen.bgcolor("darkgreen")
info_screen.title("Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡")
info_turtle = turtle.Turtle()
info_turtle.hideturtle()
info_turtle.penup()
info_turtle.color("white")
info_turtle.goto(-180, 50)
info_turtle.write("ğŸ‘¨â€ğŸ’» ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡: Ø´Ø§ÛŒØ§Ù† Ø·Ø§Ù‡Ø±Ø®Ø§Ù†ÛŒ\nğŸ“§ Ø§ÛŒÙ…ÛŒÙ„: shayanthn78@gmail.com\nğŸŒ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨: github.com/shayanthn", 
                  align="left", font=("B Nazanin", 14, "bold"))

# ğŸ”¹ Ø¯Ø±ÛŒØ§ÙØª Ù…Ø³ÛŒØ± ÙÙˆÙ†Øª `B Nazanin` Ø§Ø² ÙˆÛŒÙ†Ø¯ÙˆØ²
def get_b_nazanin_font():
    font_dir = "C:/Windows/Fonts"
    font_files = ["BNAZANIN.TTF", "B_NAZANIN.TTF", "BNazanin.ttf", "b_nazanin.ttf"]
    for font_file in font_files:
        font_path = os.path.join(font_dir, font_file)
        if os.path.exists(font_path):
            return font_path
    return None  # Ø§Ú¯Ø± ÙÙˆÙ†Øª Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯

b_nazanin_font = get_b_nazanin_font()

# ğŸ”¹ ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¯Ø± `Thread`
def analyze_emotion(face):
    global detected_emotion
    try:
        analysis = DeepFace.analyze(face, actions=['emotion'], enforce_detection=False, detector_backend='opencv')
        emotions = analysis[0]['emotion']
        detected_emotion = max(emotions, key=emotions.get)
    except Exception as e:
        detected_emotion = "Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ"

# ğŸ”¹ Ø­Ù„Ù‚Ù‡â€ŒÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§
while True:
    ret, frame = cap.read()
    if not ret:
        print("ğŸš¨ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØªØµÙˆÛŒØ± Ø§Ø² ÙˆØ¨â€ŒÚ©Ù…!")
        break

    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = face_mesh.process(rgb_frame)

    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            h, w, _ = frame.shape
            landmark_points = [(int(lm.x * w), int(lm.y * h)) for lm in face_landmarks.landmark]

            for point in landmark_points[:468]:  
                cv2.circle(frame, point, 1, (255, 0, 0), -1)  

            x_min, y_min = min(landmark_points)
            x_max, y_max = max(landmark_points)

            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 0, 255), 3)

            face_crop = frame[y_min:y_max, x_min:x_max]

            threading.Thread(target=analyze_emotion, args=(face_crop,)).start()

    # Ù†Ù…Ø§ÛŒØ´ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¯Ø± Ù¾Ù†Ø¬Ø±Ù‡â€ŒÛŒ OpenCV Ø¨Ø§ ÙÙˆÙ†Øª `B Nazanin`
    img_pil = Image.fromarray(frame)
    draw = ImageDraw.Draw(img_pil)
    if b_nazanin_font:
        try:
            font = ImageFont.truetype(b_nazanin_font, 32)
            draw.text((20, 60), f"Ø§Ø­Ø³Ø§Ø³: {detected_emotion}", font=font, fill=(0, 255, 0))
            frame = np.array(img_pil)
        except:
            cv2.putText(frame, f"Ø§Ø­Ø³Ø§Ø³: {detected_emotion}", (20, 60), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

    # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¯Ø± Ù¾Ù†Ø¬Ø±Ù‡â€ŒÛŒ `Turtle`
    emotion_turtle.clear()
    emotion_turtle.goto(-80, 0)
    emotion_turtle.write(f"Ø§Ø­Ø³Ø§Ø³: {detected_emotion}", align="center", font=("B Nazanin", 24, "bold"))

    cv2.imshow("ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª - Ø¯Ú©Ù…Ù‡ 'q' Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Ø¨Ø³ØªÙ† Ù…Ù†Ø§Ø¨Ø¹
cap.release()
cv2.destroyAllWindows()
emotion_screen.bye()
info_screen.bye()

## https://github.com/Shayanthn
## shayanthn78@gmail.com
""" ğŸš€ About the Developer
Python Expert | Advanced English Instructor | Creative Thinker

Passionate Python developer with expertise in web development, AI, automation, and data science. Skilled in problem-solving, clean code, and scalable solutions. Also an advanced English instructor with strong communication and technical documentation abilities.

ğŸ’¡ Innovator & Idea Generator | Always seeking to create efficient, cutting-edge projects.
ğŸ“Œ Proficient in Django, Flask, FastAPI, TensorFlow, Pandas, Selenium, and more.
ğŸš€ Open to collaboration on GitHub and tech-driven projects.
 """

